{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from ndlinear import NdLinear\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.transforms import autoaugment, transforms\n",
    "\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    autoaugment.AutoAugment(autoaugment.AutoAugmentPolicy.CIFAR10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    transforms.RandomErasing(p=0.2)\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', \n",
    "    train=True,\n",
    "    download=True, \n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Validation\n",
    "val_dataset.dataset.transform = test_transform\n",
    "\n",
    "# Test dataset\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', \n",
    "    train=False,\n",
    "    download=True, \n",
    "    transform=test_transform\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=128, \n",
    "    shuffle=True, \n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=128, \n",
    "    shuffle=False, \n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=128, \n",
    "    shuffle=False, \n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EnhancedNdLinearCNN(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.4):\n",
    "        super(EnhancedNdLinearCNN, self).__init__()\n",
    "        \n",
    "        # First block with residual connection\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.bn1_1 = nn.BatchNorm2d(64)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn1_2 = nn.BatchNorm2d(64)\n",
    "        self.shortcut1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=1),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Second block with residual connection\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2_1 = nn.BatchNorm2d(128)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn2_2 = nn.BatchNorm2d(128)\n",
    "        self.shortcut2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=1),\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "        \n",
    "        # Third block with residual connection\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3_1 = nn.BatchNorm2d(256)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.bn3_2 = nn.BatchNorm2d(256)\n",
    "        self.shortcut3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=1),\n",
    "            nn.BatchNorm2d(256)\n",
    "        )\n",
    "        \n",
    "        # Fourth block with residual connection\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn4_1 = nn.BatchNorm2d(512)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn4_2 = nn.BatchNorm2d(512)\n",
    "        self.shortcut4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=1),\n",
    "            nn.BatchNorm2d(512)\n",
    "        )\n",
    "        \n",
    "        # Global average pooling\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        \n",
    "        self.ndlinear1 = NdLinear(input_dims=(512,), hidden_size=(256,))\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.ndlinear2 = NdLinear(input_dims=(256,), hidden_size=(128,))\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.ndlinear3 = NdLinear(input_dims=(128,), hidden_size=(10,))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Block 1 with residual connection\n",
    "        identity = self.shortcut1(x)\n",
    "        x = F.relu(self.bn1_1(self.conv1_1(x)))\n",
    "        x = self.bn1_2(self.conv1_2(x))\n",
    "        x = F.relu(x + identity)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Block 2 with residual connection\n",
    "        identity = self.shortcut2(x)\n",
    "        x = F.relu(self.bn2_1(self.conv2_1(x)))\n",
    "        x = self.bn2_2(self.conv2_2(x))\n",
    "        x = F.relu(x + identity)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Block 3 with residual connection\n",
    "        identity = self.shortcut3(x)\n",
    "        x = F.relu(self.bn3_1(self.conv3_1(x)))\n",
    "        x = self.bn3_2(self.conv3_2(x))\n",
    "        x = F.relu(x + identity)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Block 4 with residual connection\n",
    "        identity = self.shortcut4(x)\n",
    "        x = F.relu(self.bn4_1(self.conv4_1(x)))\n",
    "        x = self.bn4_2(self.conv4_2(x))\n",
    "        x = F.relu(x + identity)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Keep your exact NdLinear structure\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.ndlinear1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x, inplace=True)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.ndlinear2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x, inplace=True)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.ndlinear3(x)\n",
    "        return x\n",
    "    \n",
    "model = EnhancedNdLinearCNN()\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load('best_model.pth', map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(models, inputs):\n",
    "    outputs = [model(inputs) for model in models]\n",
    "    outputs = torch.stack(outputs)\n",
    "    return outputs.mean(dim=0)\n",
    "\n",
    "# Train multiple models with different seeds\n",
    "models = []\n",
    "for seed in [42, 123, 456, 789, 101]:\n",
    "    torch.manual_seed(seed)\n",
    "    model = EnhancedNdLinearCNN().to(device)\n",
    "    # Train model...\n",
    "    models.append(model)\n",
    "\n",
    "# Evaluate ensemble\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = ensemble_predict(models, inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "print(f'Ensemble Accuracy: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, device, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        if (i + 1) % 50 == 0:  # Print every 50 batches\n",
    "            print(f'Epoch: {epoch+1}, Batch: {i+1}, Loss: {loss.item():.4f}, Acc: {100.*correct/total:.2f}%')\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    print(f'Training Loss: {epoch_loss:.4f}, Training Acc: {epoch_acc:.2f}%')\n",
    "    \n",
    "    # Make sure to return these values\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    val_loss = running_loss / len(dataloader)\n",
    "    val_acc = 100. * correct / total\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Validation Acc: {val_acc:.2f}%')\n",
    "    \n",
    "    # Make sure to return these values\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 50, Loss: 0.1368, Acc: 97.50%\n",
      "Epoch: 1, Batch: 100, Loss: 0.0469, Acc: 97.38%\n",
      "Epoch: 1, Batch: 150, Loss: 0.0572, Acc: 97.43%\n",
      "Epoch: 1, Batch: 200, Loss: 0.0287, Acc: 97.36%\n",
      "Epoch: 1, Batch: 250, Loss: 0.1514, Acc: 97.22%\n",
      "Epoch: 1, Batch: 300, Loss: 0.1113, Acc: 97.16%\n",
      "Epoch: 1, Batch: 350, Loss: 0.1414, Acc: 97.09%\n",
      "Training Loss: 0.0868, Training Acc: 97.08%\n",
      "Validation Loss: 0.2129, Validation Acc: 93.62%\n",
      "Current learning rate: 0.000970\n",
      "Model saved with accuracy: 93.62%\n",
      "Epoch 1/100 completed\n",
      "Best accuracy so far: 93.62%\n",
      "Epoch: 2, Batch: 50, Loss: 0.1005, Acc: 97.45%\n",
      "Epoch: 2, Batch: 100, Loss: 0.0817, Acc: 97.52%\n",
      "Epoch: 2, Batch: 150, Loss: 0.0394, Acc: 97.46%\n",
      "Epoch: 2, Batch: 200, Loss: 0.0546, Acc: 97.42%\n",
      "Epoch: 2, Batch: 250, Loss: 0.1372, Acc: 97.33%\n",
      "Epoch: 2, Batch: 300, Loss: 0.1575, Acc: 97.25%\n",
      "Epoch: 2, Batch: 350, Loss: 0.0730, Acc: 97.17%\n",
      "Training Loss: 0.0832, Training Acc: 97.16%\n",
      "Validation Loss: 0.2288, Validation Acc: 93.68%\n",
      "Current learning rate: 0.000968\n",
      "Model saved with accuracy: 93.68%\n",
      "Epoch 2/100 completed\n",
      "Best accuracy so far: 93.68%\n",
      "Epoch: 3, Batch: 50, Loss: 0.0277, Acc: 97.77%\n",
      "Epoch: 3, Batch: 100, Loss: 0.0335, Acc: 97.76%\n",
      "Epoch: 3, Batch: 150, Loss: 0.1095, Acc: 97.67%\n",
      "Epoch: 3, Batch: 200, Loss: 0.0848, Acc: 97.59%\n",
      "Epoch: 3, Batch: 250, Loss: 0.0690, Acc: 97.50%\n",
      "Epoch: 3, Batch: 300, Loss: 0.0345, Acc: 97.47%\n",
      "Epoch: 3, Batch: 350, Loss: 0.0536, Acc: 97.48%\n",
      "Training Loss: 0.0778, Training Acc: 97.48%\n",
      "Validation Loss: 0.2076, Validation Acc: 94.00%\n",
      "Current learning rate: 0.000965\n",
      "Model saved with accuracy: 94.00%\n",
      "Epoch 3/100 completed\n",
      "Best accuracy so far: 94.00%\n",
      "Epoch: 4, Batch: 50, Loss: 0.0342, Acc: 97.91%\n",
      "Epoch: 4, Batch: 100, Loss: 0.0899, Acc: 97.69%\n",
      "Epoch: 4, Batch: 150, Loss: 0.1356, Acc: 97.54%\n",
      "Epoch: 4, Batch: 200, Loss: 0.0948, Acc: 97.44%\n",
      "Epoch: 4, Batch: 250, Loss: 0.0440, Acc: 97.37%\n",
      "Epoch: 4, Batch: 300, Loss: 0.0556, Acc: 97.29%\n",
      "Epoch: 4, Batch: 350, Loss: 0.0980, Acc: 97.21%\n",
      "Training Loss: 0.0826, Training Acc: 97.21%\n",
      "Validation Loss: 0.2317, Validation Acc: 93.28%\n",
      "Current learning rate: 0.000962\n",
      "Epoch 4/100 completed\n",
      "Best accuracy so far: 94.00%\n",
      "Epoch: 5, Batch: 50, Loss: 0.0657, Acc: 97.62%\n",
      "Epoch: 5, Batch: 100, Loss: 0.0603, Acc: 97.70%\n",
      "Epoch: 5, Batch: 150, Loss: 0.0293, Acc: 97.64%\n",
      "Epoch: 5, Batch: 200, Loss: 0.0377, Acc: 97.49%\n",
      "Epoch: 5, Batch: 250, Loss: 0.0790, Acc: 97.48%\n",
      "Epoch: 5, Batch: 300, Loss: 0.0707, Acc: 97.40%\n",
      "Epoch: 5, Batch: 350, Loss: 0.1044, Acc: 97.37%\n",
      "Training Loss: 0.0793, Training Acc: 97.37%\n",
      "Validation Loss: 0.2357, Validation Acc: 93.18%\n",
      "Current learning rate: 0.000959\n",
      "Epoch 5/100 completed\n",
      "Best accuracy so far: 94.00%\n",
      "Epoch: 6, Batch: 50, Loss: 0.0683, Acc: 97.33%\n",
      "Epoch: 6, Batch: 100, Loss: 0.0128, Acc: 97.59%\n",
      "Epoch: 6, Batch: 150, Loss: 0.0928, Acc: 97.65%\n",
      "Epoch: 6, Batch: 200, Loss: 0.0977, Acc: 97.74%\n",
      "Epoch: 6, Batch: 250, Loss: 0.1500, Acc: 97.72%\n",
      "Epoch: 6, Batch: 300, Loss: 0.1117, Acc: 97.68%\n",
      "Epoch: 6, Batch: 350, Loss: 0.0250, Acc: 97.60%\n",
      "Training Loss: 0.0729, Training Acc: 97.60%\n",
      "Validation Loss: 0.2642, Validation Acc: 92.92%\n",
      "Current learning rate: 0.000956\n",
      "Epoch 6/100 completed\n",
      "Best accuracy so far: 94.00%\n",
      "Epoch: 7, Batch: 50, Loss: 0.0407, Acc: 97.50%\n",
      "Epoch: 7, Batch: 100, Loss: 0.0228, Acc: 97.73%\n",
      "Epoch: 7, Batch: 150, Loss: 0.1061, Acc: 97.69%\n",
      "Epoch: 7, Batch: 200, Loss: 0.0456, Acc: 97.59%\n",
      "Epoch: 7, Batch: 250, Loss: 0.0540, Acc: 97.54%\n",
      "Epoch: 7, Batch: 300, Loss: 0.0451, Acc: 97.49%\n",
      "Epoch: 7, Batch: 350, Loss: 0.1382, Acc: 97.44%\n",
      "Training Loss: 0.0770, Training Acc: 97.44%\n",
      "Validation Loss: 0.2345, Validation Acc: 93.52%\n",
      "Current learning rate: 0.000952\n",
      "Epoch 7/100 completed\n",
      "Best accuracy so far: 94.00%\n",
      "Epoch: 8, Batch: 50, Loss: 0.0259, Acc: 97.27%\n",
      "Epoch: 8, Batch: 100, Loss: 0.0523, Acc: 97.67%\n",
      "Epoch: 8, Batch: 150, Loss: 0.0258, Acc: 97.63%\n",
      "Epoch: 8, Batch: 200, Loss: 0.0993, Acc: 97.68%\n",
      "Epoch: 8, Batch: 250, Loss: 0.0983, Acc: 97.64%\n",
      "Epoch: 8, Batch: 300, Loss: 0.1001, Acc: 97.64%\n",
      "Epoch: 8, Batch: 350, Loss: 0.1211, Acc: 97.53%\n",
      "Training Loss: 0.0743, Training Acc: 97.52%\n",
      "Validation Loss: 0.2414, Validation Acc: 93.48%\n",
      "Current learning rate: 0.000949\n",
      "Epoch 8/100 completed\n",
      "Best accuracy so far: 94.00%\n",
      "Epoch: 9, Batch: 50, Loss: 0.0822, Acc: 97.81%\n",
      "Epoch: 9, Batch: 100, Loss: 0.1047, Acc: 97.95%\n",
      "Epoch: 9, Batch: 150, Loss: 0.0653, Acc: 97.85%\n",
      "Epoch: 9, Batch: 200, Loss: 0.0300, Acc: 97.80%\n",
      "Epoch: 9, Batch: 250, Loss: 0.0641, Acc: 97.76%\n",
      "Epoch: 9, Batch: 300, Loss: 0.0304, Acc: 97.70%\n",
      "Epoch: 9, Batch: 350, Loss: 0.0517, Acc: 97.73%\n",
      "Training Loss: 0.0684, Training Acc: 97.73%\n",
      "Validation Loss: 0.2448, Validation Acc: 93.38%\n",
      "Current learning rate: 0.000946\n",
      "Epoch 9/100 completed\n",
      "Best accuracy so far: 94.00%\n",
      "Epoch: 10, Batch: 50, Loss: 0.0779, Acc: 97.66%\n",
      "Epoch: 10, Batch: 100, Loss: 0.0232, Acc: 97.84%\n",
      "Epoch: 10, Batch: 150, Loss: 0.1490, Acc: 97.69%\n",
      "Epoch: 10, Batch: 200, Loss: 0.0999, Acc: 97.71%\n",
      "Epoch: 10, Batch: 250, Loss: 0.0169, Acc: 97.66%\n",
      "Epoch: 10, Batch: 300, Loss: 0.0250, Acc: 97.69%\n",
      "Epoch: 10, Batch: 350, Loss: 0.0749, Acc: 97.69%\n",
      "Training Loss: 0.0656, Training Acc: 97.68%\n",
      "Validation Loss: 0.2472, Validation Acc: 93.46%\n",
      "Current learning rate: 0.000942\n",
      "Epoch 10/100 completed\n",
      "Best accuracy so far: 94.00%\n",
      "Epoch: 11, Batch: 50, Loss: 0.0355, Acc: 98.00%\n",
      "Epoch: 11, Batch: 100, Loss: 0.0416, Acc: 97.87%\n",
      "Epoch: 11, Batch: 150, Loss: 0.0214, Acc: 97.74%\n",
      "Epoch: 11, Batch: 200, Loss: 0.0811, Acc: 97.72%\n",
      "Epoch: 11, Batch: 250, Loss: 0.0818, Acc: 97.71%\n",
      "Epoch: 11, Batch: 300, Loss: 0.0342, Acc: 97.72%\n",
      "Epoch: 11, Batch: 350, Loss: 0.0972, Acc: 97.72%\n",
      "Training Loss: 0.0672, Training Acc: 97.72%\n",
      "Validation Loss: 0.2537, Validation Acc: 93.38%\n",
      "Current learning rate: 0.000938\n",
      "Epoch 11/100 completed\n",
      "Best accuracy so far: 94.00%\n",
      "Epoch: 12, Batch: 50, Loss: 0.1901, Acc: 98.02%\n",
      "Epoch: 12, Batch: 100, Loss: 0.0813, Acc: 97.92%\n",
      "Epoch: 12, Batch: 150, Loss: 0.0549, Acc: 97.81%\n",
      "Epoch: 12, Batch: 200, Loss: 0.0586, Acc: 97.75%\n",
      "Epoch: 12, Batch: 250, Loss: 0.0879, Acc: 97.79%\n",
      "Epoch: 12, Batch: 300, Loss: 0.1091, Acc: 97.81%\n",
      "Epoch: 12, Batch: 350, Loss: 0.0287, Acc: 97.83%\n",
      "Training Loss: 0.0665, Training Acc: 97.82%\n",
      "Validation Loss: 0.2604, Validation Acc: 93.14%\n",
      "Current learning rate: 0.000934\n",
      "Epoch 12/100 completed\n",
      "Best accuracy so far: 94.00%\n",
      "Epoch: 13, Batch: 50, Loss: 0.0350, Acc: 97.84%\n",
      "Epoch: 13, Batch: 100, Loss: 0.0478, Acc: 97.95%\n",
      "Epoch: 13, Batch: 150, Loss: 0.0532, Acc: 98.05%\n",
      "Epoch: 13, Batch: 200, Loss: 0.0691, Acc: 98.02%\n",
      "Epoch: 13, Batch: 250, Loss: 0.0578, Acc: 98.04%\n",
      "Epoch: 13, Batch: 300, Loss: 0.0594, Acc: 98.01%\n",
      "Epoch: 13, Batch: 350, Loss: 0.1169, Acc: 97.98%\n",
      "Training Loss: 0.0628, Training Acc: 97.99%\n",
      "Validation Loss: 0.2327, Validation Acc: 93.60%\n",
      "Current learning rate: 0.000930\n",
      "Epoch 13/100 completed\n",
      "Best accuracy so far: 94.00%\n",
      "Epoch: 14, Batch: 50, Loss: 0.0486, Acc: 98.00%\n",
      "Epoch: 14, Batch: 100, Loss: 0.0230, Acc: 97.98%\n",
      "Epoch: 14, Batch: 150, Loss: 0.0667, Acc: 98.04%\n",
      "Epoch: 14, Batch: 200, Loss: 0.0819, Acc: 97.98%\n",
      "Epoch: 14, Batch: 250, Loss: 0.0390, Acc: 98.07%\n",
      "Epoch: 14, Batch: 300, Loss: 0.0711, Acc: 98.05%\n",
      "Epoch: 14, Batch: 350, Loss: 0.0229, Acc: 98.04%\n",
      "Training Loss: 0.0563, Training Acc: 98.03%\n",
      "Validation Loss: 0.2526, Validation Acc: 93.42%\n",
      "Current learning rate: 0.000926\n",
      "Epoch 14/100 completed\n",
      "Best accuracy so far: 94.00%\n",
      "Epoch: 15, Batch: 50, Loss: 0.0162, Acc: 97.75%\n",
      "Epoch: 15, Batch: 100, Loss: 0.0558, Acc: 97.84%\n",
      "Epoch: 15, Batch: 150, Loss: 0.0458, Acc: 97.91%\n",
      "Epoch: 15, Batch: 200, Loss: 0.0447, Acc: 97.85%\n",
      "Epoch: 15, Batch: 250, Loss: 0.0209, Acc: 97.84%\n",
      "Epoch: 15, Batch: 300, Loss: 0.1119, Acc: 97.73%\n",
      "Epoch: 15, Batch: 350, Loss: 0.0974, Acc: 97.77%\n",
      "Training Loss: 0.0667, Training Acc: 97.78%\n",
      "Validation Loss: 0.2541, Validation Acc: 93.26%\n",
      "Current learning rate: 0.000922\n",
      "Epoch 15/100 completed\n",
      "Best accuracy so far: 94.00%\n",
      "Epoch: 16, Batch: 50, Loss: 0.0695, Acc: 97.89%\n",
      "Epoch: 16, Batch: 100, Loss: 0.0357, Acc: 97.91%\n",
      "Epoch: 16, Batch: 150, Loss: 0.0774, Acc: 97.96%\n",
      "Epoch: 16, Batch: 200, Loss: 0.0628, Acc: 97.95%\n",
      "Epoch: 16, Batch: 250, Loss: 0.0454, Acc: 97.95%\n",
      "Epoch: 16, Batch: 300, Loss: 0.2078, Acc: 97.93%\n",
      "Epoch: 16, Batch: 350, Loss: 0.0443, Acc: 97.93%\n",
      "Training Loss: 0.0610, Training Acc: 97.93%\n",
      "Validation Loss: 0.2554, Validation Acc: 93.36%\n",
      "Current learning rate: 0.000918\n",
      "Epoch 16/100 completed\n",
      "Best accuracy so far: 94.00%\n",
      "Epoch: 17, Batch: 50, Loss: 0.0427, Acc: 97.78%\n",
      "Epoch: 17, Batch: 100, Loss: 0.0749, Acc: 97.68%\n",
      "Epoch: 17, Batch: 150, Loss: 0.0382, Acc: 97.78%\n",
      "Epoch: 17, Batch: 200, Loss: 0.0462, Acc: 97.76%\n",
      "Epoch: 17, Batch: 250, Loss: 0.1976, Acc: 97.86%\n",
      "Epoch: 17, Batch: 300, Loss: 0.0313, Acc: 97.85%\n",
      "Epoch: 17, Batch: 350, Loss: 0.0550, Acc: 97.85%\n",
      "Training Loss: 0.0617, Training Acc: 97.85%\n",
      "Validation Loss: 0.2575, Validation Acc: 93.46%\n",
      "Current learning rate: 0.000914\n",
      "Epoch 17/100 completed\n",
      "Best accuracy so far: 94.00%\n",
      "Epoch: 18, Batch: 50, Loss: 0.0419, Acc: 98.47%\n",
      "Epoch: 18, Batch: 100, Loss: 0.0687, Acc: 98.34%\n",
      "Epoch: 18, Batch: 150, Loss: 0.0523, Acc: 98.22%\n",
      "Epoch: 18, Batch: 200, Loss: 0.0498, Acc: 98.21%\n",
      "Epoch: 18, Batch: 250, Loss: 0.0765, Acc: 98.17%\n",
      "Epoch: 18, Batch: 300, Loss: 0.0499, Acc: 98.11%\n",
      "Epoch: 18, Batch: 350, Loss: 0.0164, Acc: 98.10%\n",
      "Training Loss: 0.0575, Training Acc: 98.09%\n",
      "Validation Loss: 0.2615, Validation Acc: 93.04%\n",
      "Current learning rate: 0.000909\n",
      "Epoch 18/100 completed\n",
      "Best accuracy so far: 94.00%\n",
      "Epoch: 19, Batch: 50, Loss: 0.0408, Acc: 98.33%\n",
      "Epoch: 19, Batch: 100, Loss: 0.0219, Acc: 98.38%\n",
      "Epoch: 19, Batch: 150, Loss: 0.0822, Acc: 98.39%\n",
      "Epoch: 19, Batch: 200, Loss: 0.0775, Acc: 98.37%\n",
      "Epoch: 19, Batch: 250, Loss: 0.0257, Acc: 98.30%\n",
      "Epoch: 19, Batch: 300, Loss: 0.0237, Acc: 98.25%\n",
      "Epoch: 19, Batch: 350, Loss: 0.0692, Acc: 98.20%\n",
      "Training Loss: 0.0554, Training Acc: 98.21%\n",
      "Validation Loss: 0.2625, Validation Acc: 93.16%\n",
      "Current learning rate: 0.000905\n",
      "Epoch 19/100 completed\n",
      "Best accuracy so far: 94.00%\n",
      "Epoch: 20, Batch: 50, Loss: 0.0577, Acc: 98.20%\n",
      "Epoch: 20, Batch: 100, Loss: 0.0187, Acc: 98.16%\n",
      "Epoch: 20, Batch: 150, Loss: 0.1582, Acc: 98.26%\n",
      "Epoch: 20, Batch: 200, Loss: 0.0073, Acc: 98.35%\n",
      "Epoch: 20, Batch: 250, Loss: 0.0657, Acc: 98.30%\n",
      "Epoch: 20, Batch: 300, Loss: 0.0259, Acc: 98.32%\n",
      "Epoch: 20, Batch: 350, Loss: 0.1609, Acc: 98.31%\n",
      "Training Loss: 0.0522, Training Acc: 98.30%\n",
      "Validation Loss: 0.2608, Validation Acc: 93.16%\n",
      "Current learning rate: 0.000900\n",
      "Epoch 20/100 completed\n",
      "Best accuracy so far: 94.00%\n",
      "Epoch: 21, Batch: 50, Loss: 0.0618, Acc: 98.39%\n",
      "Epoch: 21, Batch: 100, Loss: 0.0500, Acc: 98.53%\n",
      "Epoch: 21, Batch: 150, Loss: 0.0270, Acc: 98.40%\n",
      "Epoch: 21, Batch: 200, Loss: 0.0252, Acc: 98.34%\n",
      "Epoch: 21, Batch: 250, Loss: 0.0580, Acc: 98.33%\n",
      "Epoch: 21, Batch: 300, Loss: 0.1160, Acc: 98.22%\n",
      "Epoch: 21, Batch: 350, Loss: 0.0308, Acc: 98.26%\n",
      "Training Loss: 0.0539, Training Acc: 98.26%\n",
      "Validation Loss: 0.2837, Validation Acc: 92.60%\n",
      "Current learning rate: 0.000895\n",
      "Epoch 21/100 completed\n",
      "Best accuracy so far: 94.00%\n",
      "Epoch: 22, Batch: 50, Loss: 0.0378, Acc: 98.52%\n",
      "Epoch: 22, Batch: 100, Loss: 0.2104, Acc: 98.39%\n",
      "Epoch: 22, Batch: 150, Loss: 0.0344, Acc: 98.30%\n",
      "Epoch: 22, Batch: 200, Loss: 0.0437, Acc: 98.29%\n",
      "Epoch: 22, Batch: 250, Loss: 0.0197, Acc: 98.33%\n",
      "Epoch: 22, Batch: 300, Loss: 0.1012, Acc: 98.32%\n",
      "Epoch: 22, Batch: 350, Loss: 0.0500, Acc: 98.36%\n",
      "Training Loss: 0.0516, Training Acc: 98.36%\n",
      "Validation Loss: 0.2630, Validation Acc: 93.12%\n",
      "Current learning rate: 0.000890\n",
      "Epoch 22/100 completed\n",
      "Best accuracy so far: 94.00%\n",
      "Epoch: 23, Batch: 50, Loss: 0.0168, Acc: 98.47%\n",
      "Epoch: 23, Batch: 100, Loss: 0.0268, Acc: 98.32%\n",
      "Epoch: 23, Batch: 150, Loss: 0.0606, Acc: 98.30%\n",
      "Epoch: 23, Batch: 200, Loss: 0.0585, Acc: 98.29%\n",
      "Epoch: 23, Batch: 250, Loss: 0.0519, Acc: 98.26%\n",
      "Epoch: 23, Batch: 300, Loss: 0.0581, Acc: 98.31%\n",
      "Epoch: 23, Batch: 350, Loss: 0.0750, Acc: 98.29%\n",
      "Training Loss: 0.0499, Training Acc: 98.29%\n",
      "Validation Loss: 0.3259, Validation Acc: 92.12%\n",
      "Current learning rate: 0.000885\n",
      "Early stopping triggered after 23 epochs\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "best_acc = 0\n",
    "patience = 20\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device, epoch)\n",
    "    \n",
    "    # Validation phase\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f'Current learning rate: {current_lr:.6f}')\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(f'Model saved with accuracy: {best_acc:.2f}%')\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        \n",
    "    # Early stopping\n",
    "    if counter >= patience:\n",
    "        print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "        break\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs} completed')\n",
    "    print(f'Best accuracy so far: {best_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "# Load best model and evaluate\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "test_acc = test(model, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
